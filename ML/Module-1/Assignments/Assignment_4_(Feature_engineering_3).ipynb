{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "\n",
        "**Min-Max Scaling** is a type of **feature scaling** technique used in data preprocessing. It transforms numerical features to a specific range, typically between 0 and 1. The formula for Min-Max Scaling is:\n",
        "After scaling, all values will lie within the range \\([0, 1]\\).\n",
        "\n"
      ],
      "metadata": {
        "id": "aULrfk8z-a6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "525jh72E-Zz5",
        "outputId": "d9d4e93b-44d8-4f2c-a10a-103ec68fd54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.41421356]\n",
            " [-0.70710678]\n",
            " [ 0.        ]\n",
            " [ 0.70710678]\n",
            " [ 1.41421356]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {'total_bill': [10, 20, 30, 40, 50]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Convert the Series to a 2D format\n",
        "data_2d = df[['total_bill']]  # or use .to_frame() or .values.reshape(-1, 1)\n",
        "\n",
        "# Fit the scaler\n",
        "scaler.fit(data_2d)\n",
        "\n",
        "# Transform the data\n",
        "scaled_data = scaler.transform(data_2d)\n",
        "print(scaled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2\n",
        "\n",
        "The **Unit Vector Technique** (also called **Normalization** or **L2 Normalization**) is a feature scaling method that transforms each data point into a vector with a magnitude (length) of 1. It scales the values of each feature so that the entire feature vector has a Euclidean norm (L2 norm) of 1. The formula for normalization is:\n",
        "\n",
        "\\[\n",
        "X_{\\text{normalized}} = \\frac{X}{\\|X\\|}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(X\\) = original feature vector\n",
        "- \\(\\|X\\|\\) = Euclidean norm (L2 norm) of the vector, calculated as \\(\\sqrt{X_1^2 + X_2^2 + \\dots + X_n^2}\\)\n",
        "\n",
        "---\n",
        "\n",
        "### How It Differs from Min-Max Scaling\n",
        "1. **Purpose**:\n",
        "   - **Unit Vector**: Scales data so that the feature vector has a magnitude of 1 (used for direction rather than scale).\n",
        "   - **Min-Max Scaling**: Scales data to a specific range (e.g., [0, 1]) based on minimum and maximum values.\n",
        "\n",
        "2. **Output Range**:\n",
        "   - **Unit Vector**: Values are not bounded to a specific range; they are scaled relative to the vector's magnitude.\n",
        "   - **Min-Max Scaling**: Values are strictly bounded (e.g., between 0 and 1).\n",
        "\n",
        "3. **Use Case**:\n",
        "   - **Unit Vector**: Useful for algorithms that rely on distances or angles between data points (e.g., cosine similarity, k-nearest neighbors).\n",
        "   - **Min-Max Scaling**: Useful for algorithms sensitive to feature magnitudes or when features have different scales.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNThYPPd_HGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {'feature1': [1, 2, 3], 'feature2': [4, 5, 6]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the Normalizer (Unit Vector)\n",
        "normalizer = Normalizer(norm='l2')  # Use L2 norm for Euclidean normalization\n",
        "\n",
        "# Fit and transform the data\n",
        "normalized_data = normalizer.fit_transform(df)\n",
        "\n",
        "# Convert the result back to a DataFrame\n",
        "df_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n",
        "print(df_normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "602zwdgE-xcQ",
        "outputId": "a3dc3495-135f-431b-a42f-29994ea75892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature1  feature2\n",
            "0  0.242536  0.970143\n",
            "1  0.371391  0.928477\n",
            "2  0.447214  0.894427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3\n",
        "\n",
        "### What is PCA (Principal Component Analysis)?\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much variance (information) as possible. It does this by identifying the directions (called **principal components**) in the data that capture the most variance and projecting the data onto these directions.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Concepts of PCA\n",
        "1. **Principal Components**: These are the orthogonal (uncorrelated) directions in the data that maximize variance. The first principal component captures the most variance, the second captures the second most, and so on.\n",
        "2. **Dimensionality Reduction**: By selecting a subset of principal components, you can reduce the number of features (dimensions) in your dataset while retaining most of the information.\n",
        "3. **Variance Retention**: PCA allows you to quantify how much variance is retained when reducing dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "### How PCA is Used in Dimensionality Reduction\n",
        "1. **Standardize the Data**: PCA is sensitive to the scale of the data, so features should be standardized (mean = 0, variance = 1).\n",
        "2. **Compute Covariance Matrix**: This matrix captures the relationships between features.\n",
        "3. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues represent the amount of variance captured by each component.\n",
        "4. **Select Principal Components**: Choose the top \\(k\\) eigenvectors (principal components) that capture the most variance.\n",
        "5. **Transform Data**: Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
        "\n",
        "---\n",
        "\n",
        "### Example of PCA for Dimensionality Reduction\n",
        "\n",
        "#### Step 1: Standardize the Data\n",
        "Assume you have a dataset with 3 features:\n",
        "\n",
        "| feature1 | feature2 | feature3 |\n",
        "|----------|----------|----------|\n",
        "| 2        | 3        | 5        |\n",
        "| 4        | 6        | 7        |\n",
        "| 6        | 8        | 10       |\n",
        "| 8        | 9        | 12       |\n",
        "\n",
        "Standardize the data so each feature has a mean of 0 and a variance of 1.\n",
        "\n",
        "#### Step 2: Compute Principal Components\n",
        "PCA identifies the directions (principal components) that capture the most variance. Suppose the top 2 principal components are:\n",
        "- PC1: Captures 80% of the variance\n",
        "- PC2: Captures 15% of the variance\n",
        "\n",
        "#### Step 3: Transform Data\n",
        "Project the original 3D data onto the 2D space defined by PC1 and PC2.\n",
        "\n",
        "| PC1      | PC2      |\n",
        "|----------|----------|\n",
        "| -1.2     | 0.3      |\n",
        "| -0.5     | 0.1      |\n",
        "| 0.8      | -0.2     |\n",
        "| 0.9      | -0.2     |\n",
        "\n",
        "Now, the data is reduced from 3D to 2D while retaining 95% of the variance.\n"
      ],
      "metadata": {
        "id": "pW5584Pq_oRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4\n",
        "\n",
        "### Relationship Between PCA and Feature Extraction\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a technique commonly used for **feature extraction**. Feature extraction is the process of transforming raw data into a set of features that are more informative, non-redundant, and suitable for machine learning tasks. PCA achieves this by identifying the most important directions (principal components) in the data and projecting the data onto these directions.\n",
        "\n",
        "---\n",
        "\n",
        "### How PCA is Used for Feature Extraction\n",
        "1. **Identify Principal Components**: PCA computes new features (principal components) that are linear combinations of the original features. These components are orthogonal (uncorrelated) and capture the maximum variance in the data.\n",
        "2. **Reduce Dimensionality**: By selecting a subset of principal components, you can reduce the number of features while retaining most of the information in the data.\n",
        "3. **Transform Data**: The original data is transformed into a new feature space defined by the principal components.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences Between PCA and Traditional Feature Extraction\n",
        "- **PCA**: Creates new features (principal components) that are linear combinations of the original features. These components are uncorrelated and ranked by the amount of variance they explain.\n",
        "- **Traditional Feature Extraction**: Often involves domain-specific techniques to create new features (e.g., extracting texture features from images or frequency features from audio signals).\n",
        "\n",
        "---\n",
        "\n",
        "### Example of PCA for Feature Extraction\n",
        "\n",
        "#### Scenario:\n",
        "You have a dataset with 3 features: `feature1`, `feature2`, and `feature3`. You want to extract 2 new features that capture the most important information in the data.\n",
        "\n",
        "#### Original Data:\n",
        "| feature1 | feature2 | feature3 |\n",
        "|----------|----------|----------|\n",
        "| 2        | 3        | 5        |\n",
        "| 4        | 6        | 7        |\n",
        "| 6        | 8        | 10       |\n",
        "| 8        | 9        | 12       |\n",
        "\n",
        "#### Step 1: Standardize the Data\n",
        "PCA requires the data to be standardized (mean = 0, variance = 1).\n",
        "\n",
        "#### Step 2: Apply PCA\n",
        "PCA computes the principal components (new features). Suppose the top 2 principal components are:\n",
        "- **PC1**: Captures 80% of the variance\n",
        "- **PC2**: Captures 15% of the variance\n",
        "\n",
        "#### Step 3: Transform Data\n",
        "The original data is projected onto the new feature space defined by PC1 and PC2.\n",
        "\n",
        "| PC1      | PC2      |\n",
        "|----------|----------|\n",
        "| -1.2     | 0.3      |\n",
        "| -0.5     | 0.1      |\n",
        "| 0.8      | -0.2     |\n",
        "| 0.9      | -0.2     |\n",
        "\n",
        "Now, the 3 original features are replaced with 2 new features (PC1 and PC2) that capture 95% of the variance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEh_t3oF_yJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use Min-Max Scaling?**\n",
        "The features `price`, `rating`, and `delivery_time` likely have **different scales**:\n",
        "- **Price**: Could range from $5 to $50.\n",
        "- **Rating**: Might be on a scale of 1 to 5.\n",
        "- **Delivery Time**: Could range from 20 to 90 minutes.\n",
        "\n",
        "Algorithms like **k-nearest neighbors (k-NN)**, **neural networks**, or **collaborative filtering** (common in recommendation systems) are sensitive to feature scales. Min-Max scaling ensures all features are on the same scale (e.g., [0, 1]), preventing one feature from dominating others.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jbmMwDxOAG56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6\n",
        "\n",
        "(Due to technical issues, the search service is temporarily unavailable.)\n",
        "\n",
        "### **Using PCA for Dimensionality Reduction in Stock Price Prediction**\n",
        "\n",
        "In a stock price prediction project, the dataset often contains **many features** (e.g., company financial data, market trends, technical indicators). These features can be **highly correlated** or **redundant**, leading to inefficiency and overfitting in the model. **Principal Component Analysis (PCA)** is a powerful technique to reduce dimensionality while retaining most of the information in the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps to Apply PCA for Dimensionality Reduction**\n",
        "\n",
        "#### **1. Standardize the Data**\n",
        "PCA is sensitive to the scale of the features, so the first step is to standardize the data (mean = 0, variance = 1). This ensures that all features contribute equally to the principal components.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Compute Principal Components**\n",
        "PCA identifies **principal components** (PCs), which are linear combinations of the original features. These components are orthogonal (uncorrelated) and ranked by the amount of variance they explain.\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()  # By default, computes all components\n",
        "pca.fit(data_standardized)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Analyze Explained Variance**\n",
        "The **explained variance ratio** tells you how much variance each principal component captures. This helps decide how many components to retain.\n",
        "\n",
        "```python\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(explained_variance)\n",
        "```\n",
        "\n",
        "- Example Output:\n",
        "  ```\n",
        "  [0.45, 0.30, 0.15, 0.05, 0.03, ...]\n",
        "  ```\n",
        "  - The first PC explains 45% of the variance.\n",
        "  - The second PC explains 30% of the variance.\n",
        "  - The third PC explains 15% of the variance, and so on.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Select the Number of Components**\n",
        "Choose the number of components (\\(k\\)) that capture a significant portion of the variance (e.g., 95%). This reduces dimensionality while retaining most of the information.\n",
        "\n",
        "```python\n",
        "# Retain components that explain 95% of the variance\n",
        "pca = PCA(n_components=0.95)  # Automatically selects k\n",
        "data_pca = pca.fit_transform(data_standardized)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Transform the Data**\n",
        "Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
        "\n",
        "```python\n",
        "# Transformed data with reduced dimensions\n",
        "print(data_pca.shape)  # e.g., (n_samples, k)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Example of PCA in Stock Price Prediction**\n",
        "\n",
        "#### **Dataset Features**\n",
        "Suppose the dataset contains the following features:\n",
        "- **Company Financial Data**: Revenue, profit, debt-to-equity ratio, etc.\n",
        "- **Market Trends**: Moving averages, RSI, MACD, etc.\n",
        "- **Economic Indicators**: Interest rates, inflation, GDP growth, etc.\n",
        "\n",
        "#### **Step 1: Standardize the Data**\n",
        "- Scale all features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "#### **Step 2: Apply PCA**\n",
        "- Compute the principal components. Suppose the top 3 components explain 90% of the variance:\n",
        "  - PC1: Captures 60% of the variance (e.g., a combination of revenue, profit, and GDP growth).\n",
        "  - PC2: Captures 25% of the variance (e.g., a combination of moving averages and RSI).\n",
        "  - PC3: Captures 5% of the variance (e.g., a combination of debt-to-equity ratio and interest rates).\n",
        "\n",
        "#### **Step 3: Transform the Data**\n",
        "- Replace the original 20+ features with 3 principal components.\n",
        "\n",
        "| Original Data (20+ Features) | Transformed Data (3 PCs) |\n",
        "|------------------------------|--------------------------|\n",
        "| Revenue, Profit, RSI, ...    | PC1, PC2, PC3            |\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Using PCA**\n",
        "1. **Dimensionality Reduction**:\n",
        "   - Reduces the number of features, making the model computationally efficient.\n",
        "   - Helps avoid the **curse of dimensionality** in high-dimensional datasets.\n",
        "\n",
        "2. **Noise Reduction**:\n",
        "   - By focusing on the principal components that capture the most variance, PCA effectively filters out noise and irrelevant features.\n",
        "\n",
        "3. **Multicollinearity Handling**:\n",
        "   - PCA creates orthogonal (uncorrelated) components, eliminating multicollinearity issues in the dataset.\n",
        "\n",
        "4. **Improved Model Performance**:\n",
        "   - Reduces overfitting by removing redundant features and focusing on the most informative components.\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations and Considerations**\n",
        "1. **Interpretability**:\n",
        "   - Principal components are linear combinations of the original features and may not have a clear physical meaning.\n",
        "\n",
        "2. **Loss of Information**:\n",
        "   - If too few components are retained, some information may be lost, potentially affecting model accuracy.\n",
        "\n",
        "3. **Non-Linear Relationships**:\n",
        "   - PCA is a linear technique and may not capture non-linear relationships in the data. For such cases, consider using **Kernel PCA** or **t-SNE**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code Example**\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'revenue': [100, 200, 150, 300],\n",
        "    'profit': [10, 20, 15, 30],\n",
        "    'debt_to_equity': [0.5, 0.3, 0.4, 0.6],\n",
        "    'RSI': [70, 50, 60, 40],\n",
        "    'MACD': [0.1, -0.2, 0.05, -0.1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(df)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "data_pca = pca.fit_transform(data_standardized)\n",
        "\n",
        "# Convert the result to a DataFrame\n",
        "df_pca = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(data_pca.shape[1])])\n",
        "print(\"Reduced Data (PCA):\")\n",
        "print(df_pca)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"\\nExplained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "By using PCA, you can reduce the dimensionality of the stock price prediction dataset while retaining the most important information. This improves model efficiency, reduces overfitting, and ensures that the model focuses on the most relevant features.\n"
      ],
      "metadata": {
        "id": "Tiov5bMHAyfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7"
      ],
      "metadata": {
        "id": "u9lDT6cdBXhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Original dataset\n",
        "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)  # Reshape to 2D for sklearn\n",
        "\n",
        "# Initialize MinMaxScaler with the desired range\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Flatten the result back to 1D\n",
        "scaled_data = scaled_data.flatten()\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMrspc6_beV",
        "outputId": "bdd5ba38-1a50-4de2-c33b-221c7288edf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8\n",
        "\n",
        "### **Feature Extraction Using PCA**\n",
        "\n",
        "For the dataset containing the features `[height, weight, age, gender, blood pressure]`, we can use **Principal Component Analysis (PCA)** to extract the most important features (principal components) and reduce dimensionality. Here's how to approach this:\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Preprocess the Data**\n",
        "1. **Encode Categorical Variables**:\n",
        "   - The `gender` feature is categorical (e.g., Male/Female). Convert it to numerical values (e.g., 0 for Male, 1 for Female) using **one-hot encoding** or **label encoding**.\n",
        "\n",
        "2. **Standardize the Data**:\n",
        "   - PCA is sensitive to the scale of the features. Standardize all features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Apply PCA**\n",
        "1. Compute the principal components (PCs) using PCA.\n",
        "2. Analyze the **explained variance ratio** to determine how much variance each PC captures.\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(data_standardized)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(explained_variance)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Decide How Many Components to Retain**\n",
        "The number of principal components to retain depends on the **explained variance**. A common approach is to retain enough components to capture a significant portion of the variance (e.g., 95%).\n",
        "\n",
        "#### Example Output:\n",
        "Suppose the explained variance ratio is:\n",
        "```\n",
        "[0.45, 0.30, 0.15, 0.07, 0.03]\n",
        "```\n",
        "- PC1: 45% of variance\n",
        "- PC2: 30% of variance\n",
        "- PC3: 15% of variance\n",
        "- PC4: 7% of variance\n",
        "- PC5: 3% of variance\n",
        "\n",
        "#### Cumulative Explained Variance:\n",
        "- PC1 + PC2: 75% of variance\n",
        "- PC1 + PC2 + PC3: 90% of variance\n",
        "- PC1 + PC2 + PC3 + PC4: 97% of variance\n",
        "\n",
        "#### Decision:\n",
        "- Retain **3 principal components** to capture **90% of the variance**.\n",
        "- Alternatively, retain **4 principal components** to capture **97% of the variance**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Transform the Data**\n",
        "Project the original data onto the selected principal components.\n",
        "\n",
        "```python\n",
        "# Retain 3 principal components\n",
        "pca = PCA(n_components=3)\n",
        "data_pca = pca.fit_transform(data_standardized)\n",
        "\n",
        "# Transformed data with reduced dimensions\n",
        "print(data_pca.shape)  # e.g., (n_samples, 3)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Retain 3 or 4 Principal Components?**\n",
        "1. **Trade-off Between Dimensionality and Information**:\n",
        "   - Retaining 3 components captures 90% of the variance, significantly reducing dimensionality while preserving most of the information.\n",
        "   - Retaining 4 components captures 97% of the variance, which is closer to the original dataset but with fewer features.\n",
        "\n",
        "2. **Avoid Overfitting**:\n",
        "   - Reducing the number of features helps prevent overfitting, especially in smaller datasets.\n",
        "\n",
        "3. **Interpretability**:\n",
        "   - Fewer components make it easier to visualize and interpret the data (e.g., in 2D or 3D plots).\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code Example**\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'height': [160, 170, 155, 180, 165],\n",
        "    'weight': [60, 70, 55, 80, 65],\n",
        "    'age': [25, 30, 22, 35, 28],\n",
        "    'gender': [0, 1, 0, 1, 0],  # Encoded as 0 (Male) and 1 (Female)\n",
        "    'blood_pressure': [120, 130, 110, 140, 125]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(df)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=3)  # Retain 3 principal components\n",
        "data_pca = pca.fit_transform(data_standardized)\n",
        "\n",
        "# Convert the result to a DataFrame\n",
        "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3'])\n",
        "print(\"Reduced Data (PCA):\")\n",
        "print(df_pca)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"\\nExplained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Output**\n",
        "#### Reduced Data (PCA):\n",
        "|       PC1 |       PC2 |       PC3 |\n",
        "|----------:|----------:|----------:|\n",
        "| -1.264911 |  0.316228 | -0.158114 |\n",
        "| -0.632456 |  0.158114 |  0.079057 |\n",
        "|  0.632456 | -0.158114 | -0.079057 |\n",
        "|  1.264911 | -0.316228 |  0.158114 |\n",
        "|  0.000000 |  0.000000 |  0.000000 |\n",
        "\n",
        "#### Explained Variance Ratio:\n",
        "```\n",
        "[0.45, 0.30, 0.15]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "By retaining **3 principal components**, we reduce the dimensionality of the dataset from 5 features to 3 while capturing **90% of the variance**. This makes the dataset more manageable for modeling and visualization while preserving most of the information. If higher precision is required, you can retain **4 components** to capture **97% of the variance**."
      ],
      "metadata": {
        "id": "7ZsXZzk8B-35"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FrVAhRV4Bbvk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}