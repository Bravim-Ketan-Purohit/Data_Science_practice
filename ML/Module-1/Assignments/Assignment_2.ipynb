{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?**\n",
        "\n",
        "**Overfitting**:  \n",
        "Overfitting occurs when a machine learning model learns the training data too well, capturing noise and irrelevant patterns. This results in poor generalization to unseen data.  \n",
        "**Consequences**:  \n",
        "- High accuracy on training data but low accuracy on test/validation data.  \n",
        "- The model becomes overly complex and fails to generalize.  \n",
        "\n",
        "**Underfitting**:  \n",
        "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It performs poorly on both training and test data.  \n",
        "**Consequences**:  \n",
        "- Low accuracy on both training and test data.  \n",
        "- The model is unable to learn the relationships in the data.  \n",
        "\n",
        "**Mitigation**:  \n",
        "- **Overfitting**: Use techniques like regularization, cross-validation, pruning (for decision trees), dropout (for neural networks), and increasing training data.  \n",
        "- **Underfitting**: Use more complex models, add relevant features, reduce regularization, or train for more epochs.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q2: How can we reduce overfitting? Explain in brief.**\n",
        "\n",
        "To reduce overfitting:  \n",
        "1. **Regularization**: Add penalties to the loss function (e.g., L1/L2 regularization).  \n",
        "2. **Cross-Validation**: Use k-fold cross-validation to ensure the model generalizes well.  \n",
        "3. **Dropout**: In neural networks, randomly drop units during training to prevent co-adaptation.  \n",
        "4. **Early Stopping**: Stop training when validation performance stops improving.  \n",
        "5. **Simplify the Model**: Reduce model complexity (e.g., fewer layers in neural networks or fewer features).  \n",
        "6. **Increase Training Data**: More data helps the model generalize better.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**\n",
        "\n",
        "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test datasets.  \n",
        "\n",
        "**Scenarios where underfitting occurs**:  \n",
        "1. Using a linear model for a non-linear problem.  \n",
        "2. Insufficient training time (e.g., too few epochs in neural networks).  \n",
        "3. Over-regularization (e.g., high lambda in L2 regularization).  \n",
        "4. Lack of relevant features in the dataset.  \n",
        "5. Using a model with low capacity (e.g., a shallow decision tree).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?**\n",
        "\n",
        "**Bias-Variance Tradeoff**:  \n",
        "- **Bias**: Error due to overly simplistic assumptions in the model. High bias leads to underfitting.  \n",
        "- **Variance**: Error due to the model's sensitivity to small fluctuations in the training set. High variance leads to overfitting.  \n",
        "\n",
        "**Relationship**:  \n",
        "- Increasing model complexity reduces bias but increases variance.  \n",
        "- Decreasing model complexity reduces variance but increases bias.  \n",
        "\n",
        "**Effect on Model Performance**:  \n",
        "- High bias: Poor performance on both training and test data.  \n",
        "- High variance: Good performance on training data but poor performance on test data.  \n",
        "- Optimal performance is achieved by balancing bias and variance.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?**\n",
        "\n",
        "**Methods for Detection**:  \n",
        "1. **Train-Test Split**: Compare training and validation/test performance.  \n",
        "   - Overfitting: High training accuracy, low validation accuracy.  \n",
        "   - Underfitting: Low training and validation accuracy.  \n",
        "2. **Learning Curves**: Plot training and validation error over time.  \n",
        "   - Overfitting: Large gap between training and validation error.  \n",
        "   - Underfitting: Both errors are high and close to each other.  \n",
        "3. **Cross-Validation**: Use k-fold cross-validation to assess generalization.  \n",
        "\n",
        "**Determining Overfitting/Underfitting**:  \n",
        "- If the model performs well on training data but poorly on validation/test data, it is overfitting.  \n",
        "- If the model performs poorly on both training and validation/test data, it is underfitting.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?**\n",
        "\n",
        "**Comparison**:  \n",
        "- **Bias**: Error due to overly simplistic assumptions. High bias models are less flexible.  \n",
        "- **Variance**: Error due to sensitivity to small fluctuations. High variance models are overly flexible.  \n",
        "\n",
        "**Examples**:  \n",
        "- **High Bias**: Linear regression for non-linear data, shallow decision trees.  \n",
        "- **High Variance**: Deep decision trees, neural networks with too many layers.  \n",
        "\n",
        "**Performance**:  \n",
        "- High bias models underfit and perform poorly on both training and test data.  \n",
        "- High variance models overfit and perform well on training data but poorly on test data.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.**\n",
        "\n",
        "**Regularization**:  \n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. This discourages the model from fitting noise in the training data.  \n",
        "\n",
        "**Common Techniques**:  \n",
        "1. **L1 Regularization (Lasso)**: Adds the absolute value of coefficients as a penalty. Encourages sparsity (some coefficients become zero).  \n",
        "2. **L2 Regularization (Ridge)**: Adds the squared magnitude of coefficients as a penalty. Shrinks coefficients but does not zero them out.  \n",
        "3. **Elastic Net**: Combines L1 and L2 regularization.  \n",
        "4. **Dropout**: Randomly drops neurons during training in neural networks to prevent co-adaptation.  \n",
        "5. **Early Stopping**: Stops training when validation error starts increasing.  \n",
        "\n",
        "**How They Work**:  \n",
        "Regularization techniques constrain the model's complexity, forcing it to focus on the most important patterns in the data and ignore noise.  \n"
      ],
      "metadata": {
        "id": "1I8EeBNK_6y6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2z7cr-B_2a0"
      },
      "outputs": [],
      "source": []
    }
  ]
}